"""
ChatAgent - èŠå¤©Agentå®ç°

å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼Œé€šè¿‡LLMç”Ÿæˆå›å¤ï¼Œé€šè¿‡WebSocketæ¨é€
éµå¾ªæ­£ç¡®çš„Agentæ¶æ„ï¼šSense-Plan-Act-Reflect
"""
import time
import logging
import re
from typing import Any, Optional, Dict, List
from ..core.complete_agent import CompleteAgent
from ..core.agent import AgentConfig
from ..events.backend import MessageBusBackend
from ..llm.base import LLMAdapter
from ..utils.agent_logger import get_agent_logger, log_chain_start, log_chain_step, log_chain_end
from ..utils.llm_logger import get_llm_logger, log_llm_request, log_llm_response
from ..tools.registry import ToolRegistry, tool_registry
from ..tools.selector import ToolSelector
from ..tools.context_decider import ContextDecider
from ..tools.function_calling import FunctionCallingExecutor
from ..tools.schema import ToolExecutionContext
from ..memory.self_memory import SelfMemory
from ..memory.other_memory import OtherMemory
from ..memory.behavior_evolution import SatisfactionLevel
from ..memory.emotional_state import InteractionOutcome, EngagementLevel
from ..memory.growth_memory import InteractionType
from ..memory.context_builder import Scenario
from ..memory.models import TaskBehaviorProfile

logger = logging.getLogger(__name__)
agent_logger = get_agent_logger('chat')
llm_logger = get_llm_logger('chat')


def clean_tool_artifacts(text: str) -> str:
    """
    æ¸…ç†LLMå“åº”ä¸­çš„å·¥å…·è°ƒç”¨ç—•è¿¹

    ç§»é™¤ç±»ä¼¼è¿™æ ·çš„æ ¼å¼:
    - <antml:function_calls>...</antml:function_calls>
    - <antml:tool_result>...</antml:tool_result>
    - <tool_result>...</tool_result>
    - {"name": "tool", "arguments": {...}}
    - <invoke>...</invoke>

    Args:
        text: åŸå§‹LLMå“åº”

    Returns:
        æ¸…ç†åçš„å“åº”
    """
    # ç§»é™¤ function_calls æ ‡ç­¾åŠå…¶å†…å®¹
    text = re.sub(r'<antml:function_calls>.*?</antml:function_calls>', '', text, flags=re.DOTALL)

    # ç§»é™¤ tool_result æ ‡ç­¾åŠå…¶å†…å®¹
    text = re.sub(r'<antml:tool_result>.*?</antml:tool_result>', '', text, flags=re.DOTALL)
    text = re.sub(r'<tool_result>.*?</tool_result>', '', text, flags=re.DOTALL)

    # ç§»é™¤ invoke æ ‡ç­¾åŠå…¶å†…å®¹
    text = re.sub(r'<invoke>.*?</invoke>', '', text, flags=re.DOTALL)

    # ç§»é™¤ {"name": "xxx", "arguments": {...}} æ ¼å¼çš„å·¥å…·è°ƒç”¨
    text = re.sub(r'\s*{"name":\s*"[^"]+",\s*"arguments":\s*{[^}]*}}\s*', '', text)

    # ç§»é™¤å‰©ä½™çš„ç©ºè¡Œï¼ˆè¶…è¿‡2ä¸ªè¿ç»­æ¢è¡Œï¼‰
    text = re.sub(r'\n{3,}', '\n\n', text)

    return text.strip()


class ChatAgent(CompleteAgent):
    """
    èŠå¤©Agent - å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶ç”Ÿæˆå›å¤

    æ¶æ„æµç¨‹ï¼š
    1. Sense - ä»UserMessageSensorè·å–ç”¨æˆ·æ¶ˆæ¯
    2. Plan - ProcessingModuleåˆ†ææ¶ˆæ¯ï¼Œç”Ÿæˆå“åº”è®¡åˆ’
    3. Act - æ‰§è¡ŒåŠ¨ä½œï¼šè°ƒç”¨LLMç”Ÿæˆå›å¤ï¼Œé€šè¿‡WebSocketå‘é€
    4. Reflect - ä¿å­˜å¯¹è¯å†å²åˆ°è®°å¿†
    """

    def __init__(
        self,
        config: AgentConfig,
        message_bus: MessageBusBackend,
        llm_adapter: LLMAdapter,
        memory: SelfMemory = None,
        other_memory: OtherMemory = None,
        unified_memory = None,
        memory_integration = None,
    ):
        """
        åˆå§‹åŒ–ChatAgent

        Args:
            config: Agenté…ç½®
            message_bus: æ¶ˆæ¯æ€»çº¿
            llm_adapter: LLMé€‚é…å™¨
            memory: è‡ªæˆ‘è®°å¿†ç³»ç»Ÿï¼ˆå¯é€‰ï¼‰
            other_memory: ä»–äººè®°å¿†ç³»ç»Ÿï¼ˆå¯é€‰ï¼‰
            unified_memory: ç»Ÿä¸€è®°å¿†å­˜å‚¨ï¼ˆå¯é€‰ï¼ŒL1-L5ï¼‰
            memory_integration: è®°å¿†é›†æˆæ¨¡å—ï¼ˆå¯é€‰ï¼‰
        """
        super().__init__(config, message_bus, llm_adapter)

        # å¯¹è¯å†å²ï¼ˆå†…å­˜å­˜å‚¨ï¼‰
        self._conversation_history: dict[str, list[dict]] = {}

        # å·¥å…·é€‰æ‹©å™¨ï¼ˆäº”æ­¥å†³ç­–æµç¨‹ï¼‰- ä¿ç•™å…¼å®¹
        self.tool_selector = ToolSelector(
            tool_registry=tool_registry,
            llm_adapter=llm_adapter,
        )

        # ä¸Šä¸‹æ–‡å†³ç­–å™¨ - æ–°çš„å·¥å…·é€‰æ‹©æ–¹å¼
        self.context_decider = ContextDecider(
            tool_registry=tool_registry,
            llm_adapter=llm_adapter,
        )

        # å‡½æ•°è°ƒç”¨æ‰§è¡Œå™¨ - æ”¯æŒè¿ç»­å·¥å…·è°ƒç”¨
        self.function_calling_executor = FunctionCallingExecutor(
            llm_adapter=llm_adapter,
            tool_registry=tool_registry,
            skill_executor=None,  # Will set after skill_executor is initialized
        )

        # è‡ªæˆ‘è®°å¿†ç³»ç»Ÿ
        self.memory = memory

        # ä»–äººè®°å¿†ç³»ç»Ÿ
        self.other_memory = other_memory

        # ç»Ÿä¸€è®°å¿†å­˜å‚¨ï¼ˆL1-L5ï¼‰
        self.unified_memory = unified_memory

        # è®°å¿†é›†æˆæ¨¡å—
        self.memory_integration = memory_integration

        # Skill æ‰§è¡Œå™¨
        from ..skills.indexer import SkillIndexer
        from ..skills.loader import SkillLoader
        from ..skills.executor import SkillExecutor

        self._skill_indexer = SkillIndexer()
        self._skill_loader = SkillLoader(self._skill_indexer)
        self._skill_executor = SkillExecutor(self._skill_loader, llm_adapter)

        # æ›´æ–°å‡½æ•°è°ƒç”¨æ‰§è¡Œå™¨çš„skill_executor
        self.function_calling_executor.skill_executor = self._skill_executor

        # åˆå§‹åŒ– skills ç´¢å¼•
        skills = self._skill_indexer.scan_all()
        if skills:
            tool_registry.register_skill_index(skills)
            agent_logger.info(f"ğŸ“š Skills indexed: {list(skills.keys())}")

        agent_logger.info(
            f"ğŸ¤– ChatAgent initialized | Name: {config.name} | "
            f"SelfMemory: {'enabled' if memory else 'disabled'} | "
            f"OtherMemory: {'enabled' if other_memory else 'disabled'} | "
            f"UnifiedMemory: {'enabled' if unified_memory else 'disabled'} | "
            f"MemoryIntegration: {'enabled' if memory_integration else 'disabled'}"
        )

    async def execute_action(self, action: Any) -> dict:
        """
        Acté˜¶æ®µ - æ‰§è¡ŒåŠ¨ä½œ

        Args:
            action: è¦æ‰§è¡Œçš„åŠ¨ä½œï¼ˆChatResponseActionï¼‰

        Returns:
            æ‰§è¡Œç»“æœ
        """
        from ..processing.actions import ChatResponseAction

        if not isinstance(action, ChatResponseAction):
            return {"success": False, "error": "Unknown action type"}

        chain_id = action.chain_id
        user_id = action.user_id
        user_message = action.user_message

        log_chain_step(agent_logger, chain_id, "ACT", "Generating LLM response", "DEBUG")

        try:
            # ç”ŸæˆLLMå›å¤
            response_text = await self._generate_response(user_id, user_message)

            log_chain_step(
                agent_logger,
                chain_id,
                "ACT",
                f"Response generated | Length: {len(response_text)} chars",
                "DEBUG"
            )

            # æ¸…ç†å·¥å…·è°ƒç”¨ç—•è¿¹
            cleaned_response = clean_tool_artifacts(response_text)
            if cleaned_response != response_text:
                agent_logger.info("ğŸ§¹ Cleaned tool artifacts from response")
                response_text = cleaned_response

            # å‘é€å›å¤é€šè¿‡WebSocket
            from ..api.websocket import manager

            room = f"user_{user_id}"
            response_data = {
                "response": response_text,
                "timestamp": time.time(),
                "user_id": user_id,
            }

            await manager.broadcast("agent_response", response_data, room=room)

            agent_logger.info(
                f"ğŸ“¤ Response delivered | User: {user_id} | Room: {room} | Length: {len(response_text)}"
            )

            return {
                "success": True,
                "response": response_text,
                "user_id": user_id,
            }

        except Exception as e:
            agent_logger.error(f"âŒ Failed to generate response: {e}")
            return {"success": False, "error": str(e)}

    async def _generate_response(self, user_id: str, user_message: str) -> str:
        """
        ç”ŸæˆLLMå›å¤ï¼ˆé›†æˆå·¥å…·è°ƒç”¨å’ŒæŠ€èƒ½æ‰§è¡Œï¼‰

        æµç¨‹ï¼š
        1. æ£€æŸ¥æ˜¯å¦ä¸ºç›´æ¥ Skill è°ƒç”¨ (/skill-name)
        2. ä½¿ç”¨ä¸Šä¸‹æ–‡å†³ç­–å™¨é€‰æ‹©ç›¸å…³å·¥å…·
        3. ä½¿ç”¨å‡½æ•°è°ƒç”¨æ‰§è¡Œå™¨è¿›è¡Œè¿ç»­å·¥å…·è°ƒç”¨
        4. å°†æ‰§è¡Œç»“æœåé¦ˆç»™LLMç”Ÿæˆæœ€ç»ˆå›å¤

        Args:
            user_id: ç”¨æˆ·ID
            user_message: ç”¨æˆ·æ¶ˆæ¯

        Returns:
            LLMå›å¤
        """
        start_time = time.time()

        # è·å–å¯¹è¯å†å²
        history = self._conversation_history.get(user_id, [])

        # Step 0: æ£€æŸ¥æ˜¯å¦ä¸ºç›´æ¥ Skill è°ƒç”¨
        skill_invocation = self._skill_executor.validate_skill_invocation(user_message)
        if skill_invocation:
            skill_name, arguments = skill_invocation
            agent_logger.info(f"ğŸ¯ Direct skill invocation | Skill: /{skill_name} | Arguments: {arguments}")

            # æ‰§è¡Œ Skill
            skill_result = await self._execute_skill(
                skill_name,
                arguments,
                user_id,
                user_message,
                history,
            )

            if skill_result["success"]:
                return skill_result["response"]
            else:
                # Skill æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
                return f"Skill execution failed: {skill_result.get('error', 'Unknown error')}"

        # Step 1: ä¸Šä¸‹æ–‡å†³ç­– - é€‰æ‹©ç›¸å…³å·¥å…·
        import os
        import platform
        context = {
            "os": platform.system(),
            "os_version": platform.release(),
            "current_user": os.getenv("USER") or os.getenv("USERNAME") or "unknown",
            "home_dir": os.path.expanduser("~"),
            "current_dir": os.getcwd(),
        }

        context_decision = await self.context_decider.decide(user_message, context)
        agent_logger.info(
            f"ğŸ¯ Context decision | Intent: {context_decision.intent} | "
            f"Tools: {context_decision.tools} | Reasoning: {context_decision.reasoning}"
        )

        # Step 2: ä½¿ç”¨å‡½æ•°è°ƒç”¨æ‰§è¡Œå™¨å¤„ç†
        response_text = await self._generate_response_with_function_calling(
            user_id=user_id,
            user_message=user_message,
            context_decision=context_decision,
            history=history,
        )

        # è®¡ç®—ä»»åŠ¡æŒç»­æ—¶é—´
        duration = time.time() - start_time

        # ä¿å­˜åˆ°å¯¹è¯å†å²
        history.append({"role": "user", "content": user_message})
        history.append({"role": "assistant", "content": response_text})
        self._conversation_history[user_id] = history

        # è®°å½•äº¤äº’åˆ°è®°å¿†ç³»ç»Ÿ
        if self.memory:
            try:
                outcome = InteractionOutcome.SUCCESS
                await self.memory.record_interaction(
                    user_id=user_id,
                    interaction_type=InteractionType.CHAT,
                    outcome="success",
                    sentiment=0.0,
                    notes=f"Message: {user_message[:100]}..."
                )

                await self.memory.update_after_interaction(
                    outcome=outcome,
                    user_engagement=EngagementLevel.MEDIUM,
                    complexity=0.5
                )

                task_id = f"chat_{int(start_time)}_{user_id}"
                await self.memory.record_task_outcome(
                    task_id=task_id,
                    task_category=context_decision.intent,
                    user_satisfaction=SatisfactionLevel.NEUTRAL,
                    accepted=True,
                    task_complexity=0.5,
                    task_duration=duration,
                )

            except Exception as e:
                agent_logger.warning(f"Failed to record interaction: {e}")

        # æ›´æ–°ä»–äººè®°å¿†
        if self.other_memory:
            try:
                self.other_memory.update_interaction(
                    user_id=user_id,
                    interaction_type="chat",
                    outcome="positive",
                    notes=f"æ¶ˆæ¯: {user_message[:100]}{'...' if len(user_message) > 100 else ''}",
                )
            except Exception as e:
                agent_logger.warning(f"Failed to update other memory: {e}")

        return response_text

    async def _generate_response_with_function_calling(
        self,
        user_id: str,
        user_message: str,
        context_decision,
        history: list[dict],
    ) -> str:
        """
        ä½¿ç”¨å‡½æ•°è°ƒç”¨æ‰§è¡Œå™¨ç”Ÿæˆå›å¤

        Args:
            user_id: ç”¨æˆ·ID
            user_message: ç”¨æˆ·æ¶ˆæ¯
            context_decision: ä¸Šä¸‹æ–‡å†³ç­–ç»“æœ
            history: å¯¹è¯å†å²

        Returns:
            LLMå›å¤
        """
        try:
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = await self._build_system_prompt(
                tool_decision=None,  # ä¸å†éœ€è¦ tool_decision
                scenario=Scenario.CHAT,
                user_id=user_id,
                task_category=context_decision.intent,
            )

            # å¦‚æœæ²¡æœ‰é€‰æ‹©å·¥å…·ï¼Œç›´æ¥è°ƒç”¨LLM
            if not context_decision.tools:
                agent_logger.info("â„¹ï¸  No tools selected, using direct LLM response")

                messages = []
                for msg in history[-10:]:
                    messages.append(msg)
                messages.append({"role": "user", "content": user_message})

                response_text = await self._call_llm(system_prompt, messages)
                return clean_tool_artifacts(response_text)

            # ä½¿ç”¨å‡½æ•°è°ƒç”¨æ‰§è¡Œå™¨
            agent_logger.info(f"ğŸ”§ Using function calling with tools: {context_decision.tools}")

            response_text = await self.function_calling_executor.execute_with_tools(
                user_message=user_message,
                system_prompt=system_prompt,
                selected_tools=context_decision.tools,
                user_id=user_id,
                conversation_history=history,
            )

            return response_text

        except Exception as e:
            agent_logger.error(f"âŒ Error in _generate_response_with_function_calling: {e}")
            import traceback
            agent_logger.error(f"Traceback: {traceback.format_exc()}")

            # Fallback: try simple LLM call
            try:
                messages = [{"role": "user", "content": user_message}]
                system_prompt = await self._build_system_prompt(
                    tool_decision=None,
                    scenario=Scenario.CHAT,
                    user_id=user_id,
                    task_category="chat",
                )
                return await self._call_llm(system_prompt, messages)
            except Exception as e2:
                agent_logger.error(f"âŒ Fallback LLM call also failed: {e2}")
                return "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"

    async def _build_system_prompt(
        self,
        tool_decision: Optional[Dict],
        scenario: str = Scenario.CHAT,
        user_id: str = None,
        task_category: str = "chat"
    ) -> str:
        """
        æ„å»ºç³»ç»Ÿæç¤º

        Args:
            tool_decision: å·¥å…·å†³ç­–ä¿¡æ¯
            scenario: äº¤äº’åœºæ™¯
            user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼Œç”¨äºè·å–å…³ç³»ä¿¡æ¯ï¼‰
            task_category: ä»»åŠ¡ç±»åˆ«

        Returns:
            ç³»ç»Ÿæç¤º
        """
        # å¦‚æœæœ‰è®°å¿†ç³»ç»Ÿï¼Œä½¿ç”¨äººæ ¼ä¸Šä¸‹æ–‡
        if self.memory:
            try:
                personality_context = await self.memory.build_context(
                    scenario=scenario,
                    task_category=task_category,
                    user_id=user_id,
                )
                agent_logger.info(f"ğŸ­ Personality context loaded | Length: {len(personality_context)} chars")
                if personality_context:
                    agent_logger.debug(f"ğŸ­ Context preview: {personality_context[:200]}...")
            except Exception as e:
                agent_logger.warning(f"Failed to build personality context: {e}")
                personality_context = ""
        else:
            agent_logger.warning("âš ï¸ Memory system not enabled, using default personality")
            personality_context = ""

        # åŸºç¡€æç¤ºï¼ˆä¸é‡å¤èº«ä»½ä¿¡æ¯ï¼Œidentityå·²åŒ…å«åœ¨ personality_context ä¸­ï¼‰
        base_prompt = (
            "è¯·å§‹ç»ˆä»¥ä¸Šè¿°èº«ä»½å›åº”ç”¨æˆ·ã€‚"
            "ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜ã€æä¾›å»ºè®®å’Œæ‰§è¡Œä»»åŠ¡ã€‚"
        )

        # ç»„è£…æç¤º
        if personality_context:
            full_prompt = personality_context  # å·²åŒ…å«å®Œæ•´èº«ä»½ä¿¡æ¯ï¼Œä¸å†æ·»åŠ  base_prompt
        else:
            # æ²¡æœ‰äººæ ¼ä¸Šä¸‹æ–‡æ—¶çš„é»˜è®¤æç¤º
            full_prompt = (
                "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚"
                "ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜ã€æä¾›å»ºè®®å’Œæ‰§è¡Œä»»åŠ¡ã€‚"
            )

        # æ·»åŠ å·¥å…·ç›¸å…³æç¤º
        if tool_decision and tool_decision.get("tool"):
            tool_name = tool_decision.get("tool")
            full_prompt += (
                f"\n\n[ç³»ç»Ÿæç¤º] å·²ä¸ºç”¨æˆ·è°ƒç”¨å·¥å…·: {tool_name}"
                f"\nå¦‚æœä¸‹æ–¹æœ‰å·¥å…·è°ƒç”¨ç»“æœï¼Œè¯·åŸºäºç»“æœå›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
                f"\nå¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ç»“æœï¼Œè¯´æ˜å·¥å…·æ‰§è¡Œå¤±è´¥ï¼Œè¯·å‘ŠçŸ¥ç”¨æˆ·ã€‚"
            )

        return full_prompt

    async def _call_llm(self, system_prompt: str, messages: list) -> str:
        """
        è°ƒç”¨LLMç”Ÿæˆå›å¤

        Args:
            system_prompt: ç³»ç»Ÿæç¤º
            messages: æ¶ˆæ¯åˆ—è¡¨

        Returns:
            LLMå›å¤
        """
        import time
        import uuid

        # ç”Ÿæˆè¯·æ±‚ID
        request_id = str(uuid.uuid4())[:8]
        start_time = time.time()

        # åˆ¤æ–­LLMç±»å‹
        is_anthropic = hasattr(self.llm, '__class__') and 'anthropic' in self.llm.__class__.__module__.lower()
        model_name = self.llm.model_name

        # è®°å½•è¯·æ±‚æ—¥å¿—
        log_llm_request(
            llm_logger,
            request_id=request_id,
            model=model_name,
            system_prompt=system_prompt,
            messages=messages,
            max_tokens=1000,
            temperature=0.7
        )

        try:
            if is_anthropic:
                # Anthropic API: ä½¿ç”¨ system å‚æ•°
                response = await self.llm._client.messages.create(
                    model=model_name,
                    max_tokens=1000,
                    temperature=0.7,
                    system=system_prompt,
                    messages=messages,
                )
                response_text = response.content[0].text

                # æ¸…ç†å·¥å…·è°ƒç”¨ç—•è¿¹
                response_text = clean_tool_artifacts(response_text)

                # è®°å½•å“åº”æ—¥å¿—
                duration_ms = int((time.time() - start_time) * 1000)
                log_llm_response(
                    llm_logger,
                    request_id=request_id,
                    response=response_text,
                    success=True,
                    duration_ms=duration_ms
                )

                return response_text
            else:
                # OpenAI API: æ·»åŠ  system æ¶ˆæ¯åˆ°æ¶ˆæ¯åˆ—è¡¨
                full_messages = [{"role": "system", "content": system_prompt}] + messages
                response_text = await self.llm.chat(
                    messages=full_messages,
                    max_tokens=1000,
                    temperature=0.7,
                )

                # æ¸…ç†å·¥å…·è°ƒç”¨ç—•è¿¹
                response_text = clean_tool_artifacts(response_text)

                # è®°å½•å“åº”æ—¥å¿—
                duration_ms = int((time.time() - start_time) * 1000)
                log_llm_response(
                    llm_logger,
                    request_id=request_id,
                    response=response_text,
                    success=True,
                    duration_ms=duration_ms
                )

                return response_text

        except Exception as e:
            # è®°å½•é”™è¯¯æ—¥å¿—
            duration_ms = int((time.time() - start_time) * 1000)
            log_llm_response(
                llm_logger,
                request_id=request_id,
                response="",
                success=False,
                error=str(e),
                duration_ms=duration_ms
            )
            raise

    async def _execute_tool(
        self,
        tool_name: str,
        parameters: Dict[str, Any],
        user_id: str
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå·¥å…·

        Args:
            tool_name: å·¥å…·åç§°
            parameters: å·¥å…·å‚æ•°
            user_id: ç”¨æˆ·ID

        Returns:
            æ‰§è¡Œç»“æœ
        """
        try:
            # ç¡®å®šæ‰€éœ€æƒé™
            permissions = ["authenticated"]

            # æ£€æŸ¥å·¥å…·æ˜¯å¦éœ€è¦å±é™©æ“ä½œæƒé™
            tool_info = tool_registry.get_tool_info(tool_name)
            if tool_info and tool_info.get("dangerous", False):
                permissions.append("dangerous_tools")

            # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
            context = ToolExecutionContext(
                agent_id=self.config.name,  # ä½¿ç”¨ Agent é…ç½®ä¸­çš„åç§°
                user_id=user_id,
                workspace="/tmp",  # ä½¿ç”¨ /tmp ä½œä¸ºé»˜è®¤å·¥ä½œç›®å½•
                env_vars={},
                permissions=permissions,
            )

            # æ‰§è¡Œå·¥å…·
            result = await tool_registry.execute(tool_name, parameters, context)

            return {
                "success": result.success,
                "data": result.data,
                "error": result.error,
                "execution_time": result.execution_time,
            }

        except Exception as e:
            agent_logger.error(f"âŒ Tool execution error: {e}")
            return {
                "success": False,
                "error": str(e),
                "data": None,
            }

    async def _execute_skill(
        self,
        skill_name: str,
        arguments: List[str],
        user_id: str,
        user_message: str,
        conversation_history: list[dict],
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œ Skill

        Args:
            skill_name: Skill åç§°ï¼ˆä¸å¸¦ / å‰ç¼€ï¼‰
            arguments: å‘½ä»¤è¡Œå‚æ•°
            user_id: ç”¨æˆ·ID
            user_message: åŸå§‹ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²

        Returns:
            æ‰§è¡Œç»“æœ {"success": bool, "response": str, "error": ...}
        """
        import os

        # æ„å»º Skill æ‰§è¡Œä¸Šä¸‹æ–‡
        skill_context = {
            "user_id": user_id,
            "session_id": f"session_{user_id}",
            "user_message": user_message,
            "conversation_history": conversation_history,
            "env_vars": {
                "USER": os.getenv("USER") or os.getenv("USERNAME") or "unknown",
                "HOME": os.path.expanduser("~"),
                "PWD": os.getcwd(),
                "CLAUDE_SESSION_ID": f"session_{user_id}",
                "USER_ID": user_id,
            },
        }

        try:
            # æ‰§è¡Œ Skill
            result = await self._skill_executor.execute(
                skill_name=skill_name,
                arguments=arguments,
                context=skill_context,
            )

            if result.success:
                # å¦‚æœ Skill æ‰§è¡ŒæˆåŠŸï¼Œè¿”å›å†…å®¹
                response_content = result.content or ""

                # å¦‚æœ Skill è¿”å›çš„æ˜¯æŒ‡ä»¤ï¼ˆdirect modeï¼‰ï¼Œéœ€è¦é€šè¿‡ LLM ç”Ÿæˆæœ€ç»ˆå›å¤
                if result.metadata.get("mode") == "direct":
                    # ä½¿ç”¨ Skill å†…å®¹ä½œä¸ºç³»ç»Ÿæç¤º
                    system_prompt = response_content

                    messages = []
                    # æ·»åŠ å†å²å¯¹è¯
                    for msg in conversation_history[-5:]:
                        messages.append(msg)
                    # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
                    messages.append({"role": "user", "content": user_message})

                    response_text = await self._call_llm(system_prompt, messages)
                    # æ¸…ç†å·¥å…·è°ƒç”¨ç—•è¿¹
                    response_text = clean_tool_artifacts(response_text)

                    return {
                        "success": True,
                        "response": response_text,
                        "mode": "direct_with_llm",
                    }
                else:
                    # Sub-agent æ¨¡å¼ï¼Œæ¸…ç†å·¥å…·è°ƒç”¨ç—•è¿¹åè¿”å›ç»“æœ
                    response_content = clean_tool_artifacts(response_content)
                    return {
                        "success": True,
                        "response": response_content,
                        "mode": "subagent",
                    }
            else:
                return {
                    "success": False,
                    "error": result.error or "Skill execution failed",
                }

        except Exception as e:
            agent_logger.error(f"âŒ Skill execution error: {e}")
            import traceback
            agent_logger.error(f"Traceback: {traceback.format_exc()}")
            return {
                "success": False,
                "error": str(e),
            }

    def get_conversation_history(self, user_id: str) -> list[dict]:
        """
        è·å–å¯¹è¯å†å²

        Args:
            user_id: ç”¨æˆ·ID

        Returns:
            å¯¹è¯å†å²
        """
        return self._conversation_history.get(user_id, [])

    def clear_conversation_history(self, user_id: str):
        """
        æ¸…ç©ºå¯¹è¯å†å²

        Args:
            user_id: ç”¨æˆ·ID
        """
        self._conversation_history[user_id] = []
        agent_logger.info(f"ğŸ—‘ï¸ Conversation history cleared | User: {user_id}")
