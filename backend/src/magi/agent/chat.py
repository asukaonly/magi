"""
ChatAgent - èŠå¤©Agentå®ç°

å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼Œé€šè¿‡LLMç”Ÿæˆå›å¤ï¼Œé€šè¿‡WebSocketæ¨é€
éµå¾ªæ­£ç¡®çš„Agentæ¶æ„ï¼šSense-Plan-Act-Reflect
"""
import time
import logging
from typing import Any, Optional, Dict
from ..core.complete_agent import CompleteAgent
from ..core.agent import AgentConfig
from ..events.backend import MessageBusBackend
from ..llm.base import LLMAdapter
from ..utils.agent_logger import get_agent_logger, log_chain_start, log_chain_step, log_chain_end
from ..utils.llm_logger import get_llm_logger, log_llm_request, log_llm_response
from ..tools.registry import ToolRegistry, tool_registry
from ..tools.selector import ToolSelector
from ..tools.schema import ToolExecutionContext

logger = logging.getLogger(__name__)
agent_logger = get_agent_logger('chat')
llm_logger = get_llm_logger('chat')


class ChatAgent(CompleteAgent):
    """
    èŠå¤©Agent - å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶ç”Ÿæˆå›å¤

    æ¶æ„æµç¨‹ï¼š
    1. Sense - ä»UserMessageSensorè·å–ç”¨æˆ·æ¶ˆæ¯
    2. Plan - ProcessingModuleåˆ†ææ¶ˆæ¯ï¼Œç”Ÿæˆå“åº”è®¡åˆ’
    3. Act - æ‰§è¡ŒåŠ¨ä½œï¼šè°ƒç”¨LLMç”Ÿæˆå›å¤ï¼Œé€šè¿‡WebSocketå‘é€
    4. Reflect - ä¿å­˜å¯¹è¯å†å²åˆ°è®°å¿†
    """

    def __init__(
        self,
        config: AgentConfig,
        message_bus: MessageBusBackend,
        llm_adapter: LLMAdapter,
    ):
        """
        åˆå§‹åŒ–ChatAgent

        Args:
            config: Agenté…ç½®
            message_bus: æ¶ˆæ¯æ€»çº¿
            llm_adapter: LLMé€‚é…å™¨
        """
        super().__init__(config, message_bus, llm_adapter)

        # å¯¹è¯å†å²ï¼ˆå†…å­˜å­˜å‚¨ï¼‰
        self._conversation_history: dict[str, list[dict]] = {}

        # å·¥å…·é€‰æ‹©å™¨ï¼ˆäº”æ­¥å†³ç­–æµç¨‹ï¼‰
        self.tool_selector = ToolSelector(
            tool_registry=tool_registry,
            llm_adapter=llm_adapter,
        )

        agent_logger.info(f"ğŸ¤– ChatAgent initialized | Name: {config.name}")

    async def execute_action(self, action: Any) -> dict:
        """
        Acté˜¶æ®µ - æ‰§è¡ŒåŠ¨ä½œ

        Args:
            action: è¦æ‰§è¡Œçš„åŠ¨ä½œï¼ˆChatResponseActionï¼‰

        Returns:
            æ‰§è¡Œç»“æœ
        """
        from ..processing.actions import ChatResponseAction

        if not isinstance(action, ChatResponseAction):
            return {"success": False, "error": "Unknown action type"}

        chain_id = action.chain_id
        user_id = action.user_id
        user_message = action.user_message

        log_chain_step(agent_logger, chain_id, "ACT", "Generating LLM response", "DEBUG")

        try:
            # ç”ŸæˆLLMå›å¤
            response_text = await self._generate_response(user_id, user_message)

            log_chain_step(
                agent_logger,
                chain_id,
                "ACT",
                f"Response generated | Length: {len(response_text)} chars",
                "DEBUG"
            )

            # å‘é€å›å¤é€šè¿‡WebSocket
            from ..api.websocket import manager

            room = f"user_{user_id}"
            response_data = {
                "response": response_text,
                "timestamp": time.time(),
                "user_id": user_id,
            }

            await manager.broadcast("agent_response", response_data, room=room)

            agent_logger.info(
                f"ğŸ“¤ Response delivered | User: {user_id} | Room: {room} | Length: {len(response_text)}"
            )

            return {
                "success": True,
                "response": response_text,
                "user_id": user_id,
            }

        except Exception as e:
            agent_logger.error(f"âŒ Failed to generate response: {e}")
            return {"success": False, "error": str(e)}

    async def _generate_response(self, user_id: str, user_message: str) -> str:
        """
        ç”ŸæˆLLMå›å¤ï¼ˆé›†æˆå·¥å…·è°ƒç”¨ï¼‰

        æµç¨‹ï¼š
        1. å·¥å…·é€‰æ‹©ï¼ˆäº”æ­¥å†³ç­–ï¼‰
        2. å¦‚æœéœ€è¦å·¥å…·ï¼Œæ‰§è¡Œå·¥å…·å¹¶è·å–ç»“æœ
        3. å°†å·¥å…·ç»“æœåé¦ˆç»™LLMç”Ÿæˆæœ€ç»ˆå›å¤

        Args:
            user_id: ç”¨æˆ·ID
            user_message: ç”¨æˆ·æ¶ˆæ¯

        Returns:
            LLMå›å¤
        """
        # è·å–å¯¹è¯å†å²
        history = self._conversation_history.get(user_id, [])

        # Step 1: å·¥å…·é€‰æ‹©ï¼ˆäº”æ­¥å†³ç­–æµç¨‹ï¼‰
        # æ„å»ºç¯å¢ƒä¸Šä¸‹æ–‡
        import os
        import platform
        selector_context = {
            "os": platform.system(),  # Darwin, Linux, Windows
            "os_version": platform.release(),
            "current_user": os.getenv("USER") or os.getenv("USERNAME") or "unknown",
            "home_dir": os.path.expanduser("~"),  # è‡ªåŠ¨æ£€æµ‹æ­£ç¡®çš„ home ç›®å½•
            "current_dir": os.getcwd(),
        }

        tool_decision = await self.tool_selector.select_tool(user_message, selector_context)

        agent_logger.info(f"ğŸ” Tool decision result: {tool_decision}")

        tool_result = None
        if tool_decision and tool_decision.get("tool"):
            # Step 2: æ‰§è¡Œå·¥å…·
            agent_logger.info(f"ğŸ”§ Tool selected | Tool: {tool_decision['tool']} | Parameters: {tool_decision.get('parameters', {})}")

            try:
                tool_result = await self._execute_tool(
                    tool_decision["tool"],
                    tool_decision.get("parameters", {}),
                    user_id
                )

                if tool_result["success"]:
                    agent_logger.info(f"âœ… Tool executed | Result: {str(tool_result['data'])[:100]}...")
                else:
                    agent_logger.error(f"âŒ Tool failed | Error: {tool_result.get('error', 'Unknown')}")
            except Exception as e:
                agent_logger.error(f"âŒ Tool execution exception: {e}")
                import traceback
                agent_logger.error(f"Traceback: {traceback.format_exc()}")
                tool_result = {"success": False, "error": str(e), "data": None}
        else:
            agent_logger.info("â„¹ï¸  No tool selected, using direct LLM response")

        # Step 3: æ„å»ºLLMæ¶ˆæ¯
        system_prompt = self._build_system_prompt(tool_decision)

        messages = []

        # æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€è¿‘10è½®ï¼‰
        for msg in history[-10:]:
            messages.append(msg)

        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append({
            "role": "user",
            "content": user_message,
        })

        # å¦‚æœæœ‰å·¥å…·ç»“æœï¼Œæ·»åŠ å·¥å…·è°ƒç”¨ä¿¡æ¯
        if tool_result and tool_result["success"]:
            tool_info = (
                f"\n\n[å·¥å…·æ‰§è¡Œç»“æœ]\n"
                f"å·¥å…·: {tool_decision['tool']}\n"
                f"å‚æ•°: {tool_decision.get('parameters', {})}\n"
                f"æ‰§è¡Œç»“æœ: {tool_result['data']}"
            )
            messages.append({
                "role": "system",
                "content": tool_info
            })
        elif tool_decision and tool_decision.get("tool"):
            # å·¥å…·é€‰æ‹©å­˜åœ¨ä½†æ²¡æœ‰æˆåŠŸæ‰§è¡Œ
            if tool_result and not tool_result["success"]:
                error_info = (
                    f"\n\n[å·¥å…·æ‰§è¡Œå¤±è´¥]\n"
                    f"å·¥å…·: {tool_decision['tool']}\n"
                    f"é”™è¯¯: {tool_result.get('error', 'Unknown error')}"
                )
                messages.append({
                    "role": "system",
                    "content": error_info
                })

        # Step 4: è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆå›å¤
        response_text = await self._call_llm(system_prompt, messages)

        # ä¿å­˜åˆ°å¯¹è¯å†å²
        history.append({"role": "user", "content": user_message})
        history.append({"role": "assistant", "content": response_text})
        self._conversation_history[user_id] = history

        return response_text

    def _build_system_prompt(self, tool_decision: Optional[Dict]) -> str:
        """
        æ„å»ºç³»ç»Ÿæç¤º

        Args:
            tool_decision: å·¥å…·å†³ç­–ä¿¡æ¯

        Returns:
            ç³»ç»Ÿæç¤º
        """
        base_prompt = (
            "ä½ æ˜¯ Magi AI Agent Framework çš„æ™ºèƒ½åŠ©æ‰‹ã€‚"
            "ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜ã€æä¾›å»ºè®®å’Œæ‰§è¡Œä»»åŠ¡ã€‚"
            "è¯·ç”¨ç®€æ´ã€å‹å¥½çš„æ–¹å¼å›å¤ã€‚"
        )

        if tool_decision and tool_decision.get("tool"):
            tool_name = tool_decision.get("tool")
            base_prompt += (
                f"\n\n[ç³»ç»Ÿæç¤º] å·²ä¸ºç”¨æˆ·è°ƒç”¨å·¥å…·: {tool_name}"
                f"\nå¦‚æœä¸‹æ–¹æœ‰å·¥å…·è°ƒç”¨ç»“æœï¼Œè¯·åŸºäºç»“æœå›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
                f"\nå¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ç»“æœï¼Œè¯´æ˜å·¥å…·æ‰§è¡Œå¤±è´¥ï¼Œè¯·å‘ŠçŸ¥ç”¨æˆ·ã€‚"
            )

        return base_prompt

    async def _call_llm(self, system_prompt: str, messages: list) -> str:
        """
        è°ƒç”¨LLMç”Ÿæˆå›å¤

        Args:
            system_prompt: ç³»ç»Ÿæç¤º
            messages: æ¶ˆæ¯åˆ—è¡¨

        Returns:
            LLMå›å¤
        """
        import time
        import uuid

        # ç”Ÿæˆè¯·æ±‚ID
        request_id = str(uuid.uuid4())[:8]
        start_time = time.time()

        # åˆ¤æ–­LLMç±»å‹
        is_anthropic = hasattr(self.llm, '__class__') and 'anthropic' in self.llm.__class__.__module__.lower()
        model_name = self.llm.model_name

        # è®°å½•è¯·æ±‚æ—¥å¿—
        log_llm_request(
            llm_logger,
            request_id=request_id,
            model=model_name,
            system_prompt=system_prompt,
            messages=messages,
            max_tokens=1000,
            temperature=0.7
        )

        try:
            if is_anthropic:
                # Anthropic API: ä½¿ç”¨ system å‚æ•°
                response = await self.llm._client.messages.create(
                    model=model_name,
                    max_tokens=1000,
                    temperature=0.7,
                    system=system_prompt,
                    messages=messages,
                )
                response_text = response.content[0].text

                # è®°å½•å“åº”æ—¥å¿—
                duration_ms = int((time.time() - start_time) * 1000)
                log_llm_response(
                    llm_logger,
                    request_id=request_id,
                    response=response_text,
                    success=True,
                    duration_ms=duration_ms
                )

                return response_text
            else:
                # OpenAI API: æ·»åŠ  system æ¶ˆæ¯åˆ°æ¶ˆæ¯åˆ—è¡¨
                full_messages = [{"role": "system", "content": system_prompt}] + messages
                response_text = await self.llm.chat(
                    messages=full_messages,
                    max_tokens=1000,
                    temperature=0.7,
                )

                # è®°å½•å“åº”æ—¥å¿—
                duration_ms = int((time.time() - start_time) * 1000)
                log_llm_response(
                    llm_logger,
                    request_id=request_id,
                    response=response_text,
                    success=True,
                    duration_ms=duration_ms
                )

                return response_text

        except Exception as e:
            # è®°å½•é”™è¯¯æ—¥å¿—
            duration_ms = int((time.time() - start_time) * 1000)
            log_llm_response(
                llm_logger,
                request_id=request_id,
                response="",
                success=False,
                error=str(e),
                duration_ms=duration_ms
            )
            raise

    async def _execute_tool(
        self,
        tool_name: str,
        parameters: Dict[str, Any],
        user_id: str
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå·¥å…·

        Args:
            tool_name: å·¥å…·åç§°
            parameters: å·¥å…·å‚æ•°
            user_id: ç”¨æˆ·ID

        Returns:
            æ‰§è¡Œç»“æœ
        """
        try:
            # ç¡®å®šæ‰€éœ€æƒé™
            permissions = ["authenticated"]

            # æ£€æŸ¥å·¥å…·æ˜¯å¦éœ€è¦å±é™©æ“ä½œæƒé™
            tool_info = tool_registry.get_tool_info(tool_name)
            if tool_info and tool_info.get("dangerous", False):
                permissions.append("dangerous_tools")

            # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
            context = ToolExecutionContext(
                agent_id=self.config.name,  # ä½¿ç”¨ Agent é…ç½®ä¸­çš„åç§°
                user_id=user_id,
                workspace="/tmp",  # ä½¿ç”¨ /tmp ä½œä¸ºé»˜è®¤å·¥ä½œç›®å½•
                env_vars={},
                permissions=permissions,
            )

            # æ‰§è¡Œå·¥å…·
            result = await tool_registry.execute(tool_name, parameters, context)

            return {
                "success": result.success,
                "data": result.data,
                "error": result.error,
                "execution_time": result.execution_time,
            }

        except Exception as e:
            agent_logger.error(f"âŒ Tool execution error: {e}")
            return {
                "success": False,
                "error": str(e),
                "data": None,
            }

    def get_conversation_history(self, user_id: str) -> list[dict]:
        """
        è·å–å¯¹è¯å†å²

        Args:
            user_id: ç”¨æˆ·ID

        Returns:
            å¯¹è¯å†å²
        """
        return self._conversation_history.get(user_id, [])

    def clear_conversation_history(self, user_id: str):
        """
        æ¸…ç©ºå¯¹è¯å†å²

        Args:
            user_id: ç”¨æˆ·ID
        """
        self._conversation_history[user_id] = []
        agent_logger.info(f"ğŸ—‘ï¸ Conversation history cleared | User: {user_id}")
